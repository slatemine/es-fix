{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Repairing ElasticSearch\n",
    "\n",
    "Basically we managed to delete the same disk on each of 12 nodes. This had the effect of killing a lot of primary shards and leaving the cluster in an unrecoverable state. This is the script i used to bring it back to green.\n",
    "\n",
    "We've lost data - but I don't want to loose the whole index because of 1 lost primary shard. \n",
    "\n",
    "It basically uses the **_cat/shards** to work out unassigned primary and replica shards and **_cluster/reroute** to assign shards to data nodes. It works because of the **allocate_primary** flag in [**_cluster/reroute**](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html)\n",
    "\n",
    "```\n",
    "The allow_primary parameter will force a new empty primary shard to be allocated without any data. If a node which has a copy of the original shard (including data) rejoins the cluster later on, that data will be deleted: the old shard copy will be replaced by the new live shard copy.\n",
    "```\n",
    "This allows me to replace the lost primary shard with and empty one, then reasigning the replicas copies the new empty primary.    \n",
    "\n",
    "There is an extra piece that is needed, on a production cluster ElasticSearch will aggressively try to recover primary shards. You get to a point at which you cannot **reroute** primary shards because they are active.\n",
    "\n",
    "## The process\n",
    "\n",
    "- Wait for the number of allocated primary shards to stop increasing \n",
    "  ( **Warning**: reroute will result in data loss if you start too early )\n",
    "- Disable primary shard recoveries\n",
    "```\n",
    "curl -XPUT localhost:9200/_cluster/settings -d '{\n",
    "    \"transient\" : {\n",
    "        \"cluster.routing.allocation.node_initial_primaries_recoveries\" : 0\n",
    "    }\n",
    "}'\n",
    "```\n",
    "- _custer/reroute missing primary shards with **allocate_primary** set to 1, these should now all be in \"UNASSIGNED\" state\n",
    "- Optional: reassign replicas. Once the cluster is back in \"yellow\" these will proceed and be done automatically but it can be faster if you do them. In my case the nodes were repeatedly cycling through restarting shards but hitting FileNotFound exceptions. \n",
    "- Re-enable primary shard recoveries ( the default value is 4 )\n",
    "```\n",
    "curl -XPUT localhost:9200/_cluster/settings -d '{\n",
    "    \"transient\" : {\n",
    "        \"cluster.routing.allocation.node_initial_primaries_recoveries\" : 4\n",
    "    }\n",
    "}'\n",
    "```\n",
    "\n",
    "**NOTE: following script only tested with version 1.3.6 of ElasticSearch**\n",
    "\n",
    "### Further reading \n",
    " - [T37: same method but doesn't stop the primary recoveries](https://t37.net/how-to-fix-your-elasticsearch-cluster-stuck-in-initializing-shards-mode.html)\n",
    " - [ElasticSearch: Shard allocation settings](https://www.elastic.co/guide/en/elasticsearch/reference/current/shards-allocation.html)\n",
    " \n",
    "### Shard allocation settings / rolling restart\n",
    " \n",
    "Trying to resolve these issues I kept trying to disable shard allocation, as per the [rolling restart instructions](https://www.elastic.co/guide/en/elasticsearch/guide/current/_rolling_restarts.html). The problem is you cannot reroute if this setting is none.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from string import Template\n",
    "import elasticsearch\n",
    "import pandas as pd\n",
    "import io, json, random, re, sys, time\n",
    "\n",
    "client = elasticsearch.Elasticsearch([\n",
    "    {'host':'127.0.0.1:9200', 'port':9200, 'timeout'108000}\n",
    "])\n",
    "\n",
    "cat_client = elasticsearch.client.CatClient(client)\n",
    "cst_client = elasticsearch.client.ClusterClient(client) \n",
    "\n",
    "def get_shards():\n",
    "    global cat_client\n",
    "    t_str = cat_client.shards(v=True,bytes='k')\n",
    "    out = []\n",
    "    for line in t_str.split('\\n'):\n",
    "        line = re.sub(r'[ \\t]+', ',',line.strip())\n",
    "        if line.find('UNASSIGNED') != -1:\n",
    "            line = ','.join([line,',,,'])\n",
    "        elif line.find('RELOCATING') != -1:\n",
    "            continue\n",
    "        out.append(line)\n",
    "    t_str = \"\\n\".join(out)\n",
    "    shards = pd.DataFrame.from_csv( io.StringIO(t_str), index_col=[0,1] )\n",
    "    shards.sort_values(by=['prirep','state'], inplace=True)\n",
    "    shards.sort_index(inplace=True)\n",
    "    return shards\n",
    "shards = get_shards()\n",
    "shards.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I get a list of nodes and drop either **master** or **none data** from the list. I also drop **DataNode20** as it looks to have high memory already. This creates a list of nodes to which shards can be routed.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_nodes():\n",
    "    t_str = cat_client.nodes(v=True)\n",
    "    out = []\n",
    "    for line in t_str.split('\\n'):\n",
    "        line = re.sub(r'[ \\t]+', ',',line.strip())\n",
    "        out.append(line)\n",
    "    t_str = \"\\n\".join(out)\n",
    "    nodes = pd.DataFrame.from_csv( io.StringIO(t_str), index_col=[0] )\n",
    "    ignore = nodes[(nodes.master=='*') | ( nodes['node.role']!='d')]\n",
    "    ignore = ignore.append(nodes.loc['DataNode20'])\n",
    "    data_nodes = nodes.drop(ignore.index)\n",
    "    data_nodes.head(15)\n",
    "    n_list =data_nodes.index.get_values().tolist()\n",
    "    return n_list\n",
    "n_list = get_nodes()\n",
    "n_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup clients, and set to start allocations from a random point in the list. \n",
    "\n",
    "*allocate_shards* will round robin the assignment from the available data nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global i_node\n",
    "i_node = random.randint(0,len(n_list))\n",
    "\n",
    "def allocate_shards( n_index, shard, is_primary ):\n",
    "    global i_node\n",
    "    global n_list\n",
    "    global cst_client\n",
    "    \n",
    "    t_str = \"\"\"\n",
    "    {\n",
    "        \"commands\":[\n",
    "            { \"allocate\": { \n",
    "                  \"index\": \"$index\", \n",
    "                  \"shard\": $shard, \n",
    "                  \"node\": \"$node\", \n",
    "                  \"allow_primary\": $is_prim \n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    tpl = Template(t_str)\n",
    "    i_max = len(n_list)\n",
    "    params = {\n",
    "        'index': n_index,\n",
    "        'shard': shard,\n",
    "        'node': n_list[i_node%i_max],\n",
    "        'is_prim': is_primary\n",
    "    }\n",
    "    i_node +=1 \n",
    "    str = tpl.substitute(params)\n",
    "    cmd = json.loads(str)\n",
    "    s_cmd = json.dumps(cmd, indent=2)\n",
    "    #print(s_cmd)\n",
    "    cst_client.reroute(body=s_cmd)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*get_index_stats()* is basically only here to enable two tests:\n",
    " - Check we have the same number of documents before and after reroute\n",
    " - Check that we leave the index in *state=yellow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_index_stats( t_index ):\n",
    "    t_str = cat_client.indices(index=t_index, v=True)\n",
    "    out = []\n",
    "    for line in t_str.split('\\n'):\n",
    "        line = re.sub(r'[ \\t]+', ',',line.strip())\n",
    "        out.append(line)\n",
    "    t_str = \"\\n\".join(out)\n",
    "    stats = pd.DataFrame.from_csv( io.StringIO(t_str), index_col=[1] )\n",
    "    del stats['docs.deleted']\n",
    "    del stats['pri']\n",
    "    del stats['rep']\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outer loop was to allow me to run hands off. I tests without the loop for 200 or so non critical indexes, the number of docs was always the same before and after, however on my runs I had not figured out that I had to switch off *primaries_recoveries* so I kept hitting Exceptions, I was confident that when it failed there was no data loss but it needed to run hands off over night. \n",
    "\n",
    "I suspect switching off *primaries_recoveries* and only assigning primaries in a single loop would complete very quickly. \n",
    "\n",
    "Some nice points with the approach below are:\n",
    " - It runs in batches\n",
    " - It backs off if it sees and error\n",
    " - It resets on success\n",
    " - It checks shard allocation every batch\n",
    " - If there is an error it will check node allocation\n",
    "  \n",
    "If assigning replicas beware I had two cases:\n",
    " - the primary was also missing: super fast after the primary is assigned, basically a copy of empty shard\n",
    " - the primary was not missing: copying 100GB files across the cluster\n",
    "Hence the timeouts / backoffs, getting things to run hands off was tricky and the approach below could be much improved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_tries = 8\n",
    "tries = 0\n",
    "while( tries != max_tries ):\n",
    "    global n_list\n",
    "    shards = get_shards()\n",
    "    if shards.empty:\n",
    "        tries = max_tries\n",
    "        break\n",
    "    try:\n",
    "        u_shards = shards[(shards.prirep=='p') & (shards.state=='UNASSIGNED)']\n",
    "        # The following will also reassign replicas\n",
    "        # u_shards = shards[shards.state=='UNASSIGNED']\n",
    "\n",
    "        to_reroute = pd.Series(u_shards.index.get_level_values(0).unique())\n",
    "        to_reroute = to_reroute.head(3)\n",
    "\n",
    "        t = 15\n",
    "        dry_run = True\n",
    "        for index in to_reroute.get_values():\n",
    "            o_stats = get_index_stats(index)\n",
    "            if o_stats.loc[index]['health'] == 'green':\n",
    "                print(\"Nothing to do for: %s skipping \\n --- \\n\"%(index))\n",
    "                sys.stdout.flush()\n",
    "                continue\n",
    "            print(\"Repairing: %s\\n\"%(index))\n",
    "            print(o_stats)\n",
    "            a_shards = u_shards.loc[index,:]\n",
    "            print(\"\\ntodo\")\n",
    "            print(a_shards.head(10))\n",
    "            sys.stdout.flush()\n",
    "            print(\"\\nAssigning shards\")\n",
    "            # Primary shards\n",
    "            pu_shards = a_shards[(a_shards.prirep=='p') & (a_shards.state=='UNASSIGNED')]\n",
    "            for shard in pu_shards.index.get_values():\n",
    "                print(index, shard, 1)\n",
    "                sys.stdout.flush()\n",
    "                if dry_run==False:\n",
    "                    allocate_shards(index, shard, 1)\n",
    "                    time.sleep(t)\n",
    "            # Replica shards\n",
    "            ru_shards = a_shards[(a_shards.prirep=='r') & (a_shards.state=='UNASSIGNED')]\n",
    "            for shard in ru_shards.index.get_values():\n",
    "                print(index, shard, 0)\n",
    "                sys.stdout.flush()\n",
    "                if dry_run==False:\n",
    "                    allocate_shards(index, shard, 0)\n",
    "                    time.sleep(t)\n",
    "            print(\"Waiting for %ds to ensure applied before testing\"%(15))\n",
    "            sys.stdout.flush()\n",
    "            time.sleep(t)\n",
    "            print(\"\\n\")\n",
    "            n_stats = get_index_stats(index)\n",
    "            print(n_stats)\n",
    "            print(n_stats.loc[index]['health'])\n",
    "            print(\"----\")\n",
    "            assert n_stats.loc[index]['docs.count'] == o_stats.loc[index]['docs.count'] \n",
    "            assert n_stats.loc[index]['health'] != 'red'\n",
    "            tries=0\n",
    "    except Exception:\n",
    "        tries += 1\n",
    "        n_list = get_nodes()\n",
    "        if tries==max_tries:\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(tries*120)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
